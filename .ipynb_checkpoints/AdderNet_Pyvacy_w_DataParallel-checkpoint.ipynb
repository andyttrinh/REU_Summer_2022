{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Trinh/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from resnet20 import resnet20\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader \n",
    "import argparse\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_PHYSICAL_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD_DEV = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD_DEV),\n",
    "])\n",
    "\n",
    "DATA_ROOT = '../cifar10'\n",
    "\n",
    "train_dataset = CIFAR10(\n",
    "    root=DATA_ROOT, train=True, download=True, transform=transform)\n",
    "\n",
    "data_train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "test_dataset = CIFAR10(\n",
    "    root=DATA_ROOT, train=False, download=True, transform=transform)\n",
    "\n",
    "data_test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyvacy Implementation\n",
    "from pyvacy import optim, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "training_parameters = {\n",
    "    'N': len(train_dataset),\n",
    "    # An upper bound on the L2 norm of each gradient update.\n",
    "    # A good rule of thumb is to use the median of the L2 norms observed\n",
    "    # throughout a non-private training loop.\n",
    "    'l2_norm_clip': 1.0,\n",
    "    # A coefficient used to scale the standard deviation of the noise applied to gradients.\n",
    "    'noise_multiplier': 1.1,\n",
    "    # Each example is given probability of being selected with minibatch_size / N.\n",
    "    # Hence this value is only the expected size of each minibatch, not the actual. \n",
    "    'minibatch_size': 128,\n",
    "    # Each minibatch is partitioned into distinct groups of this size.\n",
    "    # The smaller this value, the less noise that needs to be applied to achieve\n",
    "    # the same privacy, and likely faster convergence. Although this will increase the runtime.\n",
    "    'microbatch_size': 1,\n",
    "    # The usual privacy parameter for (ε,δ)-Differential Privacy.\n",
    "    # A generic selection for this value is 1/(N^1.1), but it's very application dependent.\n",
    "    'delta': 1e-5,\n",
    "    # The number of minibatches to process in the training loop.\n",
    "    'iterations': 15000,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "minibatch_size = 16\n",
    "microbatch_size = 1\n",
    "iterations = 200\n",
    "l2_norm_clip = 0.7\n",
    "noise_multiplier = 1.\n",
    "delta = 1e-5\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet20().cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.DPSGD(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    params=net.parameters(),\n",
    "    lr=lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Parallelism\n",
    "# net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "checkpoint = torch.load('./checkpoints/AdderNet_DPSGD.pth')\n",
    "net.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch_check = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "acc_best = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    lr = 0.05 * (1+math.cos(float(epoch)/400*math.pi))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    global cur_batch_win\n",
    "    net.train()\n",
    "    loss_list, batch_list = [], []\n",
    "    for i, (images, labels) in enumerate(data_train_loader):\n",
    "        images, labels = Variable(images).cuda(), Variable(labels).cuda()\n",
    " \n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        output = net(images)\n",
    " \n",
    "        loss = criterion(output, labels)\n",
    " \n",
    "        loss_list.append(loss.data.item())\n",
    "        batch_list.append(i+1)\n",
    " \n",
    "        if i == 1:\n",
    "            print('Train - Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.data.item()))\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    global acc, acc_best\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    avg_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(data_test_loader):\n",
    "            images, labels = Variable(images).cuda(), Variable(labels).cuda()\n",
    "            output = net(images)\n",
    "            avg_loss += criterion(output, labels).sum()\n",
    "            pred = output.data.max(1)[1]\n",
    "            total_correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    " \n",
    "    avg_loss /= len(test_dataset)\n",
    "    acc = float(total_correct) / len(test_dataset)\n",
    "    if acc_best < acc:\n",
    "        acc_best = acc\n",
    "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.data.item(), acc))\n",
    "    return (avg_loss.data.item(), acc)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Epoch', 'Loss', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, save_path, epoch):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(epoch):\n",
    "    curr_list = [epoch]\n",
    "    train(epoch)\n",
    "    loss, acc = test()\n",
    "    curr_list.append(loss)\n",
    "    curr_list.append(acc)\n",
    "    df.loc[len(df.index)] = curr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epoch_check, epoch):\n",
    "        train_and_test(e)\n",
    "        if e % 2 == 0:\n",
    "            save_checkpoint(net, optimizer, \"./checkpoints/AdderNet_DPSGD.pth\", e)\n",
    "            df.to_csv('./checkpoints/AdderNet_DP.csv')\n",
    "#         torch.save(net.state_dict(),'./AdderNet_DPSGD.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'./AdderNet_DPSGD.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result/AdderNet_DP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
